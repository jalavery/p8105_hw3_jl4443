---
title: "Homework 3"
author: "Jessica Lavery"
date: "10/10/2019"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(kableExtra)
library(tidyverse)
library(p8105.datasets)
```

# Problem 1
```{r}
# load the dataset of interest
data("instacart")

# look at overview of dataset
# str(instacart)

# calculate the number of items per order
n_per_order <- instacart %>% 
  group_by(user_id, order_id) %>% 
  summarize(n_items = n())

```

The instacart dataset is in the structure of 1 record per item per order and contains `r nrow(instacart)` records pertaining to information on `r n_distinct(pull(instacart, user_id))` users and `r n_distinct(pull(instacart, order_id))` oders. 

There are `r ncol(instacart)` variables containing key information such as a customer ID, the number of days since their prior order (truncated at 30 days), a product identifier and indication of whether that item has been purchased before, the name of the product as well as the aisle and department that it can be found, as well as the time of the instacart purchase. 
An example of a few products from a single customer's order can be seen below:

```{r}
head(instacart) %>% kable()
```

1. How many aisles are there, and which aisles are the most items ordered from?

There is data on products from `r n_distinct(pull(instacart, aisle_id))` aisles in the instacart dataset. 

As shown below, the most items are ordered from the fresh vegetables aisle, followed by the fresh fruits aisle. 

```{r}
# count number of items purchased from each aisle
items_per_aisle <- instacart %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) 

items_per_aisle %>% 
  arrange(desc(n_items))
```

The fewest items are ordered from the beauty aisle, followed by the frozen juice and baby accessories aisles. 

```{r}
items_per_aisle %>% 
  arrange(n_items)
```

2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10,000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r}
items_per_aisle %>% 
  filter(n_items >= 10000) %>% 
  mutate(aisle = fct_reorder(factor(str_to_sentence(aisle)), n_items)) %>% 
  ggplot(aes(x = aisle, y = n_items)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Number of items ordered in each aisle",
       x = "Aisle",
       y = "Number of items", 
       caption = "Note: Aisles with fewer than 10,000 items purchased were excluded from this figure.") +
  coord_flip()
```

3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

Table. Top 3 most popular items in select aisles.

```{r}

# count number of times each item within each aisle was purchased, then get top 3 items per aisle
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>%
  summarize(n_times_ordered = n()) %>%
  group_by(aisle) %>%
  top_n(3, n_times_ordered) %>% 
  kable()
```

4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

Table. Average hour purchased by day of week that Pink Lady Apples and Coffee Ice Cream were purchased.

```{r}

# calculate mean order hour by product (apples/ice cream) and day of week
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_order_hour = round(mean(order_hour_of_day), 2)) %>%
  pivot_wider(names_from = order_dow,
              values_from = mean_order_hour) %>% 
  kable()
```

# Problem 2

```{r}
# read in the BRFSS data
data("brfss_smart2010") 

# clean variable names and manipulate select variables
brfss <- brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr,
         state_county = locationdesc) %>% 
  filter(topic == "Overall Health" & response %in% c("Excellent", "Fair", "Good", "Poor")) %>% 
  mutate(response = factor(response, levels = c("Poor", "Good", "Fair", "Excellent")))

str(brfss)
```

1. In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r}
n_locations <- brfss %>% 
  group_by(year, state) %>% 
  summarize(n_locations = n_distinct(geo_location)) %>% 
  filter(n_locations >= 7) %>% 
  arrange(year, desc(n_locations))
```

In 2002, the following states were observed at 7 or more locations: 

```{r}
n_locations %>% 
  filter(year == 2002) 
```

In 2010, the following states were observed at 7 or more locations: 

```{r}
n_locations %>% 
  filter(year == 2010)
```

2. Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
excellent_brfss <- brfss %>% 
  filter(response == "Excellent") %>% 
  group_by(year, state) %>% 
  summarize(avg_data_value = mean(data_value)) %>% 
  select(year, state, avg_data_value)
  
ggplot(data = excellent_brfss, aes(x = year, y = avg_data_value, group = state)) +
  geom_line() +
  labs(x = "Year",
       y = "Average data value",
       title = "Spaghetti plot of average data values among excellent responses across locations within states")

```

3. Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
brfss %>% 
  filter(year %in% c(2006, 2010), state == "NY") %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() + 
  labs(x = "Response",
       y = "Distribution of data value",
       title = "Distribution of data value by response and year among locations in New York State") + 
  facet_wrap(~ year)
```

# Problem 3

```{r}

# read in data, clean variable names, and wrangle from wide to long format
accel <- read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(weekday = case_when(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "Weekday",
                             day %in% c("Saturday", "Sunday") ~ "Weekend"),
         day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  pivot_longer(cols = starts_with("activity"),
               names_to = "minute",
               names_prefix = "activity_",
               values_to = "activity_count") %>% 
  mutate(minute = as.numeric(minute))

# look at structure of the data - make sure all are a reasonable class
# str(accel)
```

The accelerometer dataset includes data on activity counts for each minute of a 24-hour day collected on a 63 year-old male who was admitted to Columbia University Medical Center. There are `r ncol(accel)` variables in the tidied dataset that indicate information about when the data was recorded (week, day of week, minute of day, day of study) and the activity count recorded by the accelerometer. For this patient we have `r n_distinct(pull(accel, day_id))` days of data collected by the accelerometer.

1. Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}
accel_by_day <- accel  %>% 
  group_by(day_id) %>% 
  summarize(total_activity = sum(activity_count)) 
```

Looking at the total amount of activity per day, no obvious trends are apperent.  The total amount of activity on a given day ranges from `r min(pull(accel_by_day, total_activity))` to `r max(pull(accel_by_day, total_activity))`, and on average is `r `r mean(pull(accel_by_day, total_activity))` (standard deviation `r `r sd(pull(accel_by_day, total_activity))`).

```{r}
accel_by_day %>% 
  kable()
```

2. Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.


##left off here -- finish graph and then describe patterns/conclusions

```{r}
accel %>% 
  mutate(hour = minute %/% 60) %>% 
  group_by(day, hour) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  ggplot(aes(x = hour, y = total_activity, color = day)) +
  geom_line() +
  labs(x = "Hour",
       y = "Total activity",
       title = "Total activity recorded by day of observation") +
  scale_y_continuous(limits = c(0, 2500))
```

